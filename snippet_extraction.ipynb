{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "snippet_extraction.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_oMzhxu5IVY"
      },
      "source": [
        "**Importing All the Libraries required**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0NrCO6AHRwtj",
        "outputId": "6755c3a6-fd6c-4cfa-d145-d51262c865d8"
      },
      "source": [
        "#importing all the modules\n",
        "from sklearn import svm\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer #TF-IDF evaluates how relevant a word is to a document in a collection of documents.\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "\n",
        "!pip install goslate\n",
        "!pip install googletrans==3.1.0a0\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
        "#spacy model,re, Beautifulsoup\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.7/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (57.0.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "Requirement already satisfied: goslate in /usr/local/lib/python3.7/dist-packages (1.5.1)\n",
            "Requirement already satisfied: futures in /usr/local/lib/python3.7/dist-packages (from goslate) (3.1.1)\n",
            "Requirement already satisfied: googletrans==3.1.0a0 in /usr/local/lib/python3.7/dist-packages (3.1.0a0)\n",
            "Requirement already satisfied: httpx==0.13.3 in /usr/local/lib/python3.7/dist-packages (from googletrans==3.1.0a0) (0.13.3)\n",
            "Requirement already satisfied: hstspreload in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2020.12.22)\n",
            "Requirement already satisfied: idna==2.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2.10)\n",
            "Requirement already satisfied: rfc3986<2,>=1.3 in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (1.5.0)\n",
            "Requirement already satisfied: httpcore==0.9.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (0.9.1)\n",
            "Requirement already satisfied: chardet==3.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (3.0.4)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2021.5.30)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (1.2.0)\n",
            "Requirement already satisfied: h11<0.10,>=0.8 in /usr/local/lib/python3.7/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (0.9.0)\n",
            "Requirement already satisfied: h2==3.* in /usr/local/lib/python3.7/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (3.2.0)\n",
            "Requirement already satisfied: hyperframe<6,>=5.2.0 in /usr/local/lib/python3.7/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (5.2.0)\n",
            "Requirement already satisfied: hpack<4,>=3.0 in /usr/local/lib/python3.7/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (3.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSPZBm7SHhmy"
      },
      "source": [
        "#**Function to determine Determine the Answer type of the given Telugu Query**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfycbqU7SDXd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "2cb78c37-3f6f-4b28-b190-69634dea2460"
      },
      "source": [
        "#The model in this function defines the ner tag for given question\n",
        "def Question_Classification(user_input):\n",
        "  qa=open(\"/content/drive/MyDrive/Odqa_telugu_Train_set.txt\",\"a\")\n",
        "  qa.write(\"    :\"+user_input+\"\\n\")\n",
        "  qa.close()\n",
        "  f=open(\"/content/drive/MyDrive/Odqa_telugu_Train_set.txt\",\"r\")\n",
        "  classes=[]\n",
        "  queries=[]\n",
        "  new_label=[]\n",
        "  for line in f:\n",
        "\n",
        "    line=line.rstrip('\\n')\n",
        "    classes.append((line.split()[0]).split(\":\")[0])\n",
        "    lb=(line.split()[0]).split(\":\")[0]\n",
        "    #print(lb)\n",
        "    if(lb==\"PERS\"):\n",
        "      new_label.append(1)\n",
        "    if(lb==\"LOCA\"):\n",
        "      new_label.append(2)\n",
        "    if(lb==\"ORGA\"):\n",
        "      new_label.append(3)\n",
        "    if(lb==\"DATE\"):\n",
        "      new_label.append(4)\n",
        "    if(lb==\"TIME\"):\n",
        "      new_label.append(5)\n",
        "    if(lb==\"PERC\"):\n",
        "      new_label.append(6)\n",
        "    if(lb==\"NUMB\"):\n",
        "      new_label.append(7)\n",
        "    if(lb==\"CURR\"):\n",
        "      new_label.append(8)\n",
        "    queries.append(line[5:])\n",
        "\n",
        "  vectorizer =TfidfVectorizer(min_df=1)# min_df is 1, which means \"ignore terms that appear in less than 1 document\". Thus, the default setting does not ignore any terms.\n",
        "  \n",
        "  X = vectorizer.fit_transform(classes) #Hence text converted to vectors to process\n",
        "  Y = vectorizer.fit_transform(queries)\n",
        "\n",
        "  X_ = Y[:len(new_label)]\n",
        "  Y_ = new_label[:len(new_label)]\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X_, Y_, test_size=0.25, random_state=42)\n",
        "#\tprint(\"X_train = \",X_train.shape)\n",
        "#\tprint(\"X_test = \",X_test.shape)\n",
        "\n",
        "\n",
        "  test=Y[len(queries)-1].toarray()\n",
        "  \n",
        "\n",
        "\n",
        "  #nn=MLPClassifier(activation='tanh',solver='sgd',hidden_layer_sizes=(80,50,30,8),random_state=4,alpha=0.1,batch_size=71)#   73.9\n",
        "  nn = LinearSVC(random_state=0, tol=0.3,loss=\"squared_hinge\",multi_class=\"crammer_singer\")# 76.648\n",
        "  #nn = LogisticRegression(random_state=0, solver='sag', multi_class='multinomial')# 73.9\n",
        "  \n",
        "\n",
        "\t\n",
        "\n",
        "  nn.fit(X_train,y_train)\n",
        "  pred2=nn.predict(X_test)\n",
        "\n",
        "\n",
        "  hits=0.00\n",
        "  for i in range(0,len(y_test)):\n",
        "    if y_test[i]==pred2[i]:\n",
        "      hits=hits+1\n",
        "  print(\"The accuracy is \",((hits/len(y_test))*100.0))\n",
        "  \n",
        "  \n",
        "  \n",
        "\t\n",
        "\n",
        "  result=nn.predict(test)\n",
        "  \n",
        "  if(result==1):\n",
        "    ans_type.append(\"PERSON\")\n",
        "\n",
        "  elif(result==2):\n",
        "    ans_type.append(\"LOCATION\")\n",
        "\n",
        "  elif(result==3):\n",
        "    ans_type.append(\"ORGANIZATION\")\n",
        "\n",
        "  elif(result==4):\n",
        "    ans_type.append(\"DATE\")\n",
        "\n",
        "  elif(result==5):\n",
        "    ans_type.append(\"TIME\")\n",
        "\n",
        "  elif(result==6):\n",
        "    ans_type.append(\"PERCENTAGE\")\n",
        "  \n",
        "  elif(result==7):\n",
        "    ans_type.append(\"NUMBER\")\n",
        "  else:\n",
        "    ans_type.append(\"CURRENCY\")\n",
        "  return(ans_type[0])\n",
        "user_input_telugu=\"“అడ్వెంచర్స్ ఆఫ్ టామ్ సాయర్” రచయిత ఎవరు?\" #input(\"enter query\")\n",
        "ans_type=[]\n",
        "Question_Classification(user_input_telugu)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The accuracy is  75.54945054945054\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'PERSON'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qzxt7re-5TWr"
      },
      "source": [
        "#**Language Translation section**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4appurjyUi3",
        "outputId": "93dc03c5-81c8-486d-fb8e-55fb1581ae6e"
      },
      "source": [
        "#TO TRANSLATE LIST OF QUESTIONS AND LIST OF ANSWERS FROM TELUGU TO ENGLISH\n",
        "def translate(quest_list,ans_list):\n",
        "  from googletrans import Translator\n",
        "  translater=Translator()\n",
        "  out_quest=[] #tranlated list\n",
        "  out_ans=[]\n",
        "  for quest in quest_list:\n",
        "    translated=translater.translate(quest,dest=\"en\")\n",
        "    out_quest.append(translated.text)\n",
        "  for ans in ans_list:\n",
        "    translated=translater.translate(ans,dest=\"en\")\n",
        "    out_ans.append(translated.text)\n",
        "  return out_quest,out_ans\n",
        "(Target_Questions,Target_Answers)=translate([\"భారత దేశ ప్రధాని ఎవరు?\",\"అడ్వెంచర్స్ ఆఫ్ టామ్ సాయర్” రచయిత ఎవరు?\"],[\"మోడీ\"])\n",
        "print(Target_Questions,\"and\",Target_Answers)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Who is the Prime Minister of India?', \"Who is the author of 'Adventures of Tom Sawyer'?\"] and ['Modi']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKWUU1t667RQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6665bc0a-d9e3-4a66-c8da-c61aa29f6499"
      },
      "source": [
        "#TO TRANSLATE SINGLE QUESTION FROM TELUGU TO ENGLISH\n",
        "def translate_2(to_translate, to_langage=\"auto\", langage=\"auto\"):\n",
        "        import goslate\n",
        "        gs = goslate.Goslate()\n",
        "     #   print(gs.translate(to_translate, 'en'))\n",
        "        result = gs.translate(to_translate,'en')\n",
        "        return result\n",
        "translate_2(\"భారత దేశ ప్రాధాని ఎవరు?\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Who is the Prime of India?'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZ9LMitk4nb0",
        "outputId": "61c21df4-4bd0-4e5d-9e8c-fb372c9ad706"
      },
      "source": [
        "#To translate a FINAL ANSWER from English to Telugu\n",
        "def translate_eng_to_tel(ans):\n",
        "  from googletrans import Translator\n",
        "  translater=Translator()\n",
        "  out=translater.translate(ans,dest=\"te\")\n",
        "  print(out.text)\n",
        "  return out.text\n",
        "print(translate_eng_to_tel(\"country\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "దేశం\n",
            "దేశం\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GC6AsoHh5eH5"
      },
      "source": [
        "#**Translating whole File which is in Telugu to English**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qe4F7uLlmKQR"
      },
      "source": [
        "def translate_file():\n",
        "  t = open(\"/content/drive/MyDrive/odqa_telugu_testset.txt\",\"r\").readlines()\n",
        "  print(len(t))\n",
        "  Target_Questions_telugu = []#All the Questions(which are in telugu) in test set will be copied into this list\n",
        "  Target_Answers_telugu\t = []#All the corresponding Answers are copied into this list\n",
        "\n",
        "  #copying the contents in the test set to above two lists\n",
        "  for line in t:\n",
        "    line=line.strip()\n",
        "    line=line.split(\"@@@\")\n",
        "    Target_Questions_telugu.append(line[0].strip())\n",
        "    Target_Answers_telugu.append(line[1].strip())\n",
        "  \n",
        "  #Now we translate whole list of Questions and Answers into which are in Telugu to English using below code\n",
        "  (Target_questions,Target_answers)=translate(Target_Questions_telugu,ans_list)# translate funcion defined above\n",
        "\n",
        "  #Now we store Tranlsated questions and Answers which are in english in a new file named \"odqa_telugu_testset.te.en.text\"\n",
        "  f = open(\"/content/drive/MyDrive/odqa_telugu_testset.te_en.txt\",\"w\")\n",
        "  for i in range(0,len(Target_questions)):\n",
        "    f.write(Target_questions[i]+\" \"+Target_answers[i]+\"\\n\")\n",
        "  f.close()\n",
        "#Now we are ready with test set in telugu for Question Classificationa and test set english for web scraping\n",
        "\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7--kp4r6JPy"
      },
      "source": [
        "**Performing NER tagging for Extracted text**\n",
        "#This function outputs possible answers list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rkVkp0DHSObD",
        "outputId": "c6b82027-e305-4c25-e254-12ad3758cef1"
      },
      "source": [
        "def ner_tag(sents,tag):\n",
        "  import spacy \n",
        "  from spacy import displacy\n",
        "  if(tag==\"LOCATION\"):\n",
        "    tag=\"GPE\"\n",
        "  if(tag==\"NUMBER\"):\n",
        "    tag=\"CARDINAL\"\n",
        "  if(tag==\"ORGANIZATION\"):\n",
        "    tag=\"ORG\"\n",
        "  if(tag==\"CURRENCY\"):\n",
        "    tag=\"MONEY\"\n",
        "\n",
        "  \n",
        "\n",
        "  # Load SpaCy model\n",
        "  nlp = spacy.load(\"en_core_web_sm\")\n",
        "  \n",
        "  import pandas as pd\n",
        "  \n",
        "  doc = nlp(sents)\n",
        "\n",
        "  entities = []\n",
        "  labels = []\n",
        "  position_start = []\n",
        "  position_end = []\n",
        "\n",
        "  for ent in doc.ents:\n",
        "    entities.append(ent)\n",
        "    labels.append(ent.label_)\n",
        "    position_start.append(ent.start_char)\n",
        "    position_end.append(ent.end_char)\n",
        "    #print(entities, labels)\n",
        "  \n",
        "  ans=[]  \n",
        "  df = pd.DataFrame({'Entities':entities,'Labels':labels}) #,'Position_Start':position_start, 'Position_End':position_end})\n",
        "  for i in range(0,len(labels)):\n",
        "    if labels[i]==tag:\n",
        "      ans.append(entities[i])\n",
        "  print(df)\n",
        "\n",
        "  \n",
        "\n",
        "    \n",
        "  print(\"possible answers list is\",ans)\n",
        "  #print(\"final answer is\",ans[0])\n",
        "  #final_ans=ans[0]\n",
        "      \n",
        "\n",
        "\n",
        "\n",
        " \n",
        "  return ans\n",
        "#ner_tag(\"narendra modi on 2-03-2021  gave 1 billion dollars, RS.900, $351 AND 500 dollars supreme court is prime minister  Rs 540 crore of Lom Turkish Togo  lira of india\",\"ORGANIZATION\") \n",
        "#ner_tag(\"Lome 1650\",\"LOCATION\")\n",
        "ner_tag(\"\\n Mark  Twain, MARK TWAIN AND  Narendra modi money 500 dollars should NOT exceed INR 4000 USD 100 per annum.\",\"CURRENCY\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                Entities   Labels\n",
            "0             (Narendra)      ORG\n",
            "1  (money, 500, dollars)    MONEY\n",
            "2                  (INR)      ORG\n",
            "3       (4000, USD, 100)  PRODUCT\n",
            "possible answers list is [money 500 dollars]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[money 500 dollars]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8HfLTVn7na_"
      },
      "source": [
        "#**Funciton to find Sequence Matching SCORE for two strings**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00hfERY8iW8R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ec4814e-bb8c-4218-ffbd-c2dcfe538286"
      },
      "source": [
        "\n",
        "def similarity(str1,str2):\n",
        "  str1=str1.lower()\n",
        "  str2=str2.lower()\n",
        "  #print(str1,str2)\n",
        "  \n",
        "  from difflib import SequenceMatcher\n",
        "  score=SequenceMatcher(lambda x: x==\" \",str1,str2).ratio()\n",
        "  \n",
        "  #print(score)\n",
        "  return score\n",
        "similarity(\"venu\",\"Vanu\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.75"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVNnhrzX8ARm"
      },
      "source": [
        "#**Special NER tagger for CURRENCY type**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9z6F4W9diSgr",
        "outputId": "acae73dd-42c0-4be0-8669-1fe8d4532719"
      },
      "source": [
        "def curr_ner(words_list):\n",
        "  #print(words_list)\n",
        "  ans_list=[]\n",
        "  \n",
        "  train=['Turkish lira','Turkish','lira','Birr', 'peso', 'Metical', 'Mark', 'Balboa', 'Ruble', 'Euro', 'Krona', 'Lake', 'Colon', 'Euro', 'Sudanese pound', 'Kina', 'Corona', 'Turkish lira', 'Deutsche Mark', 'Malaysian Dollar', 'Kuwaiti Dinar', 'Inti Soul', 'Drachma', 'pound', 'Bolivar', 'Real', 'New Shekel', 'peso', 'Colon', 'pound', 'Tugrick', 'French Frank', 'American Dollar', 'Shilling', 'Rs. 1000', 'American Dollar', 'Rs 2 crore', 'Rs 540 crore', 'Rs. 115', 'Rs.5,000','₹','₹1234567890','750-million-dollar', '50 pence', 'Rs.650 crore', '1 billion', 'USD 75 million', 'Rs.50,000 crores', 'Rs 2 trillion', 'Rs 2 trillion', 'Euro', '1,01,500', '1,8292 crores', 'USD 15 million']\n",
        "  #print(train)\n",
        "  i=0\n",
        "  for word in words_list:\n",
        "    for word_in_train in train:\n",
        "      sc=similarity(word_in_train,word)\n",
        "      #print(word_in_train,word,sc)\n",
        "      if(sc>=0.7):\n",
        "        ans_list.append(word)\n",
        "    i+=1\n",
        "  print(\"Possible answers list is \",ans_list)\n",
        "  return (ans_list)\n",
        "curr_ner(['Birr',\"nothing\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Possible answers list is  ['Birr']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Birr']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0k5sdMP8Xrl"
      },
      "source": [
        "#**Preprocessing Extracted Data (so that spacy ner tagger works well on extracted data)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "D73Mr5GtQ5M-",
        "outputId": "b99659e7-61e3-46e1-d83a-8e864864587b"
      },
      "source": [
        "# Extracted data not having space between the words. so it is becoming difficult for spacy ner tagger to identify the tags.\n",
        "#This function takes that unspaced extracted data as input and outputs data with spaces between the words\n",
        "import re #For Regular expressions.\n",
        "\n",
        "def editing(phrase):\n",
        "  \n",
        "  #print(phrase)\n",
        "  new_phrase=\"\"\n",
        "  j=0\n",
        "  pattern=re.compile(r'[A-Z][a-z]+')\n",
        "  \n",
        "  list=[]\n",
        " \n",
        "  new_phrase=phrase\n",
        "  k=0\n",
        "  for i in pattern.finditer(phrase):\n",
        "    #print(\"true\")\n",
        "    #print(new_phrase[i.start()-1+k:i.start()+k])\n",
        "    if(new_phrase[i.start()-1+k:i.start()+k]!=\" \"):\n",
        "      new_phrase=new_phrase[0:i.start()+k]+\" \"+new_phrase[i.start()+k:]\n",
        "      k+=1\n",
        "  print(\"preprocessed extracted text is:::\",new_phrase)\n",
        "  return new_phrase\n",
        "find(\"YES! ItIf1895Mark Twain  1650 AD worked \")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "preprocessed extracted text is::: YES! It If1895 Mark Twain  1650 AD worked \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'YES! It If1895 Mark Twain  1650 AD worked '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUr-ymIi9qo2"
      },
      "source": [
        "## **Main code starts here**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_ZFWf2RRlRl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5625f6cc-b021-47b2-a954-fdd8fa29aa09"
      },
      "source": [
        "#MAIN CODE\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "import requests, lxml, os\n",
        "from nltk import sent_tokenize\n",
        "from nltk import word_tokenize\n",
        "\n",
        "headers = {\n",
        "    'User-agent':\n",
        "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.102 Safari/537.36 Edge/18.19582\"\n",
        "}\n",
        "\n",
        "proxies = {\n",
        "  'http': os.getenv('HTTP_PROXY')\n",
        "}\n",
        "\n",
        "t = open(\"/content/drive/MyDrive/odqa_telugu_testset.txt\",\"r\").readlines()\n",
        "print(len(t))\n",
        "Target_Questions_telugu = []#All the Questions(which are in telugu) in test set will be copied into this list\n",
        "Target_Answers_telugu\t = []#All the corresponding Answers((which are in telugu) are copied into this list\n",
        "\n",
        "#copying the contents in the test set to above two lists\n",
        "for line in t:\n",
        "  line=line.strip()\n",
        "  line=line.split(\"@@@\")\n",
        "  Target_Questions_telugu.append(line[0].strip()[5:])\n",
        "  Target_Answers_telugu.append(line[1].strip())\n",
        "\n",
        "t2 = open(\"/content/drive/MyDrive/odqa_telugu_testset.te.en.txt\",\"r\").readlines()\n",
        "print(len(t2))\n",
        "Target_Questions = [] #This list will have all the Question in ENGLISH language\n",
        "Target_Answers\t = [] #This list will have all the Answers in ENGLISH language\n",
        "\n",
        "for line in t2:\n",
        "  line=line.strip()\n",
        "  line=line.split(\"?\")\n",
        "  Target_Questions.append(line[0].strip()[5:]+\"?\")#copying only question not the ans type whih is at 0 to 4 indexes\n",
        "  Target_Answers.append(line[1].strip())\n",
        "\n",
        "\n",
        "#t = open(\"/content/drive/MyDrive/odqa_telugu_testset_english.txt\",\"r\").readlines()\n",
        "\n",
        "n=int(input(\"no of questions to be searched\"))\n",
        "hit=0\n",
        "partial=0\n",
        "partial_2=0\n",
        "help=0 #int(input(\"ENTER starting position of tag(like 250 for curr,200 for number,150 for date,100 for org,50 for loc,0 for pers\"))\n",
        "\n",
        "help2=\"\"#input(\"enter helping tag\")\n",
        "#(Target_Questions,Target_Answers)=translate(Target_Questions_telugu[0:n],Target_Answers_telugu[0:n]) # Test questions and Test answers are converted from telugu to english\n",
        "#print(\"checking for conversion to english from telugu\")\n",
        "print(Target_Questions[0:n])\n",
        "#print(Target_Answers[0:n])\n",
        "\n",
        "z=0\n",
        "count=0\n",
        "final_answer_list=[]\n",
        "for z in range(0,n):\n",
        "  if(z>=0 and z<50):#These conditions useful to find Quesion classification accuracy\n",
        "    help2=\"PERSON\"\n",
        "  if(z>=50 and z<100):\n",
        "    help2=\"LOCATION\"\n",
        "  if(z>=100 and z<150):\n",
        "    help2=\"ORGANIZATION\"\n",
        "  if(z>=150 and z<200):\n",
        "    help2=\"DATE\"\n",
        "  if(z>=200 and z<250):\n",
        "    help2=\"NUMBER\"\n",
        "  if(z>=250 and z<300):\n",
        "   help2=\"CURRENCY\"  \n",
        "  z=z+help\n",
        "\n",
        "  print(\"\\nsearching for question\",z+1,\"..............,\")\n",
        "  print(\"Question: \",Target_Questions[z])\n",
        "\n",
        "  \n",
        "  user_input_telugu=Target_Questions_telugu[z]\n",
        "  user_input=Target_Questions[z]    #input(\"Enter your Query in telugu \")\n",
        "  \n",
        "  ans_type=[]\n",
        "  tag=Question_Classification(user_input_telugu)\n",
        "  print(tag)\n",
        "  \n",
        "  if(tag==help2):#Counting number of correct Question Classification. \"count\" variable used to find accuracy of QC at the end of this code\n",
        "    count+=1\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "  user_input_english=Target_Questions[z] #input('search in english: ')\n",
        "  #user_input_english=translate_2(user_input_telugu2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  #user_input_english=translate_2(user_input_telugu2)\n",
        "  google_search=requests.get(\"https://www.google.com/search?q=\"+user_input_english, headers=headers, proxies=proxies)\n",
        "  #google_search= requests.get('https://scholar.google.com/scholar?hl=en&as_sdt=0%2C5&q='+user_input_english)\n",
        "  #print(google_search.text)\n",
        "  soup=BeautifulSoup(google_search.content,'html.parser')\n",
        "\n",
        "  #print(\"########################################################################################################\")\n",
        "  #print(\"text file is\",soup.text)\n",
        "  snippet=\"\"\n",
        "  for s in soup.findAll(id='res'):\n",
        "    #print(s.)\n",
        "    print('true')\n",
        "    \n",
        "    s=s.text.replace(\"Search ResultsFeatured snippet from the web\",\" \").split(\"(\")[0]\n",
        "    ns=s.replace(\"Search ResultsWeb results\",\"no snippet: \")\n",
        "    data=ns\n",
        "  print(data)\n",
        "    \n",
        "  final_data=find(data)\n",
        "\n",
        "\n",
        "  try:\n",
        "    if(tag=='CURRENCY'):\n",
        "      print(\"OKKKKK\")\n",
        "      words_list=word_tokenize(final_data)\n",
        "      \n",
        "      ans=curr_ner(words_list)\n",
        "    else:\n",
        "      ans=ner_tag(final_data,tag)\n",
        "    #print(df)\n",
        "    \n",
        "    \n",
        "\n",
        "  except:\n",
        "    ans=[\"NO ANSWER\"]\n",
        "    \n",
        "    pass\n",
        "  k=1\n",
        "  try:\n",
        "    final_ans=ans[0]\n",
        "  \n",
        "    while(user_input_english.editing(str(final_ans))!=-1):\n",
        "      \n",
        "      final_ans=str(ans[k])\n",
        "      k+=1\n",
        "  except:\n",
        "    final_ans=\"NO ANSWER\"\n",
        "    \n",
        "\n",
        "  print('final answer is', final_ans)\n",
        "  print('final answer in telugu is ',translate_eng_to_tel(final_ans))\n",
        "  print(\"Actual answer is\",Target_Answers[z])\n",
        "  final_answer_list.append(str(final_ans))\n",
        "\n",
        "  similar={}\n",
        "  target_ans_words=set()\n",
        "  final_ans_words=set()\n",
        "  target_ans_words=set(word_tokenize(Target_Answers[z]))\n",
        "  final_ans_words=set(word_tokenize(str(final_ans)))\n",
        "  rvector=set()\n",
        "  rvector = target_ans_words.union(final_ans_words) \n",
        "  v1=[] #for target answers\n",
        "  v2=[]\n",
        "  \n",
        "  for w in rvector:\n",
        "    if w in target_ans_words:\n",
        "      v1.append(1)\n",
        "    else:\n",
        "      v1.append(0)\n",
        "    if w in final_ans_words:\n",
        "      v2.append(1)\n",
        "    else:\n",
        "      v2.append(0)\n",
        "  c=0\n",
        "  j=0\n",
        "  try:\n",
        "    for j in range(len(rvector)):\n",
        "      c+=v1[j]*v2[j]\n",
        "    v1_square=[v**2 for v in v1]\n",
        "    v2_square=[v**2 for v in v2]\n",
        "      \n",
        "    cosine2=c/(float((sum(v1_square)*sum(v2_square))**0.5))\n",
        "    print(\"cosine similarity is \",cosine2)\n",
        "     \n",
        "  except:\n",
        "    pass\n",
        "  score=similarity(Target_Answers[z],str(final_ans))\n",
        "  print(\"Sequence matcher score is \",score)\n",
        "  if cosine2==1.0 or score==1.0:\n",
        "    hit+=1\n",
        "  if cosine2>=0.7 or score>=0.7:\n",
        "    partial+=1\n",
        "  if cosine2>=0.4 or score>=0.5:\n",
        "    partial_2+=1\n",
        "  print(\"hits are \",hit,partial,partial_2)\n",
        "  print(\"#################################################\")\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "accuracy= hit*100/n\n",
        "print(\"actual answers list is,\",Target_Answers[0:n])\n",
        "print(\"answers that I got are\",final_answer_list)\n",
        "print(\"Exact match accuracy for\",n,\"no of questions is \",accuracy)\n",
        "\n",
        "#print(\"no of partial correct ans are \",partial)\n",
        "partial_accuracy=partial*100/n\n",
        "print(\"partial match accurccy is for \",n,\"no of questions is \",partial_accuracy)\n",
        "\n",
        "#print(\"no of partial_2(cosine>=0.5) are \",partial_2)\n",
        "partial_2_accuracy=partial_2*100/n\n",
        "print(\"partial_2 accuracy(cosinesimilarity>=0.5) is for \",n,\"no of questioons is \",partial_2_accuracy)\n",
        "#print(\"total count is\",count)\n",
        "print(\"Quesiton classification accuracy is \",(count*100)/n)\n",
        "\n",
        "\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "300\n",
            "300\n",
            "no of questions to be searched3\n",
            "[\" Who is the author of 'Adventures of Tom Sawyer'?\", \" Who is the author of 'Kavya Narthaki'?\", \" Who is the author of 'Don Juan'?\"]\n",
            "\n",
            "searching for question 1 ..............,\n",
            "Question:   Who is the author of 'Adventures of Tom Sawyer'?\n",
            "PERSON\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "true\n",
            "Search ResultsThe Adventures of Tom Sawyer/AuthorMark Twain\n",
            " Search Results The Adventures of Tom Sawyer/ Author Mark Twain\n",
            "        Entities  Labels\n",
            "0  (Mark, Twain)  PERSON\n",
            "possible answers list is [Mark Twain]\n",
            "final answer is Mark Twain\n",
            "మార్క్ ట్వైన్\n",
            "final answer in telugu is  మార్క్ ట్వైన్\n",
            "Actual answer is Mark Twain\n",
            "cosine similarity is  1.0\n",
            "Sequence matcher score is  1.0\n",
            "hits are  1 1 1\n",
            "#################################################\n",
            "\n",
            "searching for question 2 ..............,\n",
            "Question:   Who is the author of 'Kavya Narthaki'?\n",
            "PERSON\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "true\n",
            "no snippet: Changampuzha Krishna Pillai - Wikipediahttps://en.wikipedia.org › wiki › Changampuzha_Krish...https://en.wikipedia.org › wiki › Changampuzha_Krish...Occupation, Poet, writer. Language, Malayalam. Alma mater, Maharaja's College, Ernakulam. Period, 1931–1948. Genre, Romantic. Notable works, Ramanan. Spouse, Sreedevi Amma. Children, Sreekumar, Ajitha, Jayadev, Lalitha. Changampuzha Krishna Pillai \n",
            "no snippet: Changampuzha Krishna Pillai - Wikipediahttps://en.wikipedia.org › wiki › Changampuzha_ Krish...https://en.wikipedia.org › wiki › Changampuzha_ Krish... Occupation, Poet, writer. Language, Malayalam. Alma mater, Maharaja's College, Ernakulam. Period, 1931–1948. Genre, Romantic. Notable works, Ramanan. Spouse, Sreedevi Amma. Children, Sreekumar, Ajitha, Jayadev, Lalitha. Changampuzha Krishna Pillai \n",
            "                             Entities    Labels\n",
            "0     (Changampuzha, Krishna, Pillai)    PERSON\n",
            "1  (Krish...https://en.wikipedia.org)  CARDINAL\n",
            "2                         (Malayalam)       ORG\n",
            "3             (Maharaja, 's, College)       ORG\n",
            "4                           (Ramanan)       GPE\n",
            "5                    (Sreedevi, Amma)    PERSON\n",
            "6                            (Ajitha)      NORP\n",
            "7                           (Jayadev)       GPE\n",
            "8                           (Lalitha)    PERSON\n",
            "9     (Changampuzha, Krishna, Pillai)    PERSON\n",
            "possible answers list is [Changampuzha Krishna Pillai, Sreedevi Amma, Lalitha, Changampuzha Krishna Pillai]\n",
            "final answer is Changampuzha Krishna Pillai\n",
            "చాంగంపూజ కృష్ణ పిళ్ళై\n",
            "final answer in telugu is  చాంగంపూజ కృష్ణ పిళ్ళై\n",
            "Actual answer is Changampuzha Krishna Pillai\n",
            "cosine similarity is  1.0\n",
            "Sequence matcher score is  1.0\n",
            "hits are  2 2 2\n",
            "#################################################\n",
            "\n",
            "searching for question 3 ..............,\n",
            "Question:   Who is the author of 'Don Juan'?\n",
            "PERSON\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "true\n",
            " Lord ByronLord Byron derived the character, but not the story, from the Spanish legend of Don Juan....Don Juan \n",
            " Lord Byron Lord Byron derived the character, but not the story, from the Spanish legend of Don Juan.... Don Juan \n",
            "               Entities  Labels\n",
            "0  (Byron, Lord, Byron)  PERSON\n",
            "1             (Spanish)    NORP\n",
            "2           (Don, Juan)  PERSON\n",
            "3           (Don, Juan)  PERSON\n",
            "possible answers list is [Byron Lord Byron, Don Juan, Don Juan]\n",
            "final answer is Byron Lord Byron\n",
            "బైరాన్ లార్డ్ బైరాన్\n",
            "final answer in telugu is  బైరాన్ లార్డ్ బైరాన్\n",
            "Actual answer is Lord Byron\n",
            "cosine similarity is  1.0\n",
            "Sequence matcher score is  0.38461538461538464\n",
            "hits are  3 3 3\n",
            "#################################################\n",
            "actual answers list is, ['Mark Twain', 'Changampuzha Krishna Pillai', 'Lord Byron']\n",
            "answers that I got are ['Mark Twain', 'Changampuzha Krishna Pillai', 'Byron Lord Byron']\n",
            "Exact match accuracy for 3 no of questions is  100.0\n",
            "partial match accurccy is for  3 no of questions is  100.0\n",
            "partial_2 accuracy(cosinesimilarity>=0.5) is for  3 no of questioons is  100.0\n",
            "Quesiton classification accuracy is  100.0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}